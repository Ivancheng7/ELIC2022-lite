# 项目建设文档

## 项目概述

## 功能模块

## 技术架构

### 知识蒸馏方案
1. **蒸馏方向一：Cheng2020Attention同模型减少层数蒸馏**
   - **模型选择**: 使用Cheng2020Attention作为基础模型，减少网络层数构建学生模型
   - **蒸馏策略**: 采用MGD方法进行特征蒸馏，重点对齐中间层特征
   - **预期效果**: 保持模型性能同时减少计算复杂度

2. **蒸馏方向二：Cheng2020Anchor同模型减少层数蒸馏**
   - **模型选择**: 使用Cheng2020Anchor作为基础模型，减少网络层数构建学生模型
   - **蒸馏策略**: 采用MGD方法进行特征蒸馏，重点关注锚点特征对齐
   - **预期效果**: 保持压缩效率同时降低模型参数量

3. **蒸馏方向三：跨模型蒸馏(Cheng2020Attention→Cheng2020Anchor)**
   - **模型选择**: Cheng2020Attention作为教师模型，Cheng2020Anchor作为学生模型
   - **蒸馏策略**: 采用MGD方法进行跨模型特征蒸馏，使用1x1卷积对齐特征维度
   - **预期效果**: 结合教师模型的高性能和学生模型的高效性

4. **通用蒸馏配置**
   - **损失函数设计**: 采用混合损失函数(MSE + 知识蒸馏损失)
     - **MSE损失**: 计算学生网络输出与真实图像的差异
     - **知识蒸馏损失**: 由两部分组成
       - **生成图MSE损失**: 学生与教师网络生成图的MSE差异(权重β)
       - **MGD特征损失**: 使用Masked Generative Distillation方法对齐中间特征(权重1-β)
     - **超参数**: β控制两种蒸馏损失的权重(默认0.5)，总和为1
   - **训练策略**: 分阶段训练，先蒸馏特征再微调

## 数据准备
### DIV2K数据集使用说明
- **数据划分**: 训练集800张图像，验证集100张图像，测试集100张图像
- **预处理方法**: 
  - 随机裁剪为128x128大小（参考Deep-Lossy-Plus-Residual-Coding-main项目中的extract_patches_train.py和extract_patches_valid.py脚本实现）
  - 随机水平翻转
  - 归一化到[0,1]范围
- **评估指标**: 
  - PSNR (峰值信噪比)
  - MS-SSIM (多尺度结构相似性)
  - 压缩率 (bpp)

## 部署说明

## 优化方案
1. 采用模块化文档结构便于维护
2. 添加可视化图表说明架构
3. 包含多语言版本支持