# 第二章 相关理论与技术基础

本章将系统梳理图像压缩领域的理论基础与关键技术，重点突出深度学习方法和知识蒸馏在现代图像压缩中的作用。🔄

## 2.1 传统图像压缩技术概述

在图像压缩技术的发展历程中，离散余弦变换（DCT）为代表的传统方法长期占据主导地位。以JPEG标准为例，其核心思想是将图像从空间域映射到频率域，结合量化与编码步骤，有效削减冗余信息，实现高效压缩。由于实现简便且效率突出，这类方法广泛应用于实际生产与生活场景。🔄

## 2.2 基于深度学习的图像压缩方法

随着深度学习的兴起，图像压缩领域迎来变革。不同于传统方法，深度学习模型如自编码器（AE）、变分自编码器（VAE）、生成对抗网络（GAN）、注意力机制及Transformer架构等，通过端到端训练，能够自动提取并重建图像特征。自编码器实现高效编码与重建，VAE引入概率建模增强潜在空间表达，GAN提升重建图像的感知质量，而注意力机制和Transformer则显著增强了模型对图像全局与局部特征的建模能力。🔄

值得强调的是，Ballé等人首次提出基于卷积神经网络（CNN）的端到端学习型图像压缩（LIC）模型[4]，极大推动了该领域进步。 subsequent，研究者们将VAE与超先验机制结合，进一步捕捉空间依赖性并提升压缩效率[4]。Minnen等人[34]提出的局部上下文自适应自回归熵建模方法，则有效提高了压缩率。

🔄注意力机制在深度学习图像压缩中同样扮演着重要角色。非本地注意力模块[42]已被集成进LIC架构，能够帮助模型聚焦于关键区域，提升细节还原与速率失真性能，但其计算量较大。为兼顾效率与性能，学者们提出了简化的本地注意力模块[8]以及基于窗口的注意力机制[29]，这些方法在加速推理的同时，依然保持了较好的压缩表现。

## 2.3 熵编码与概率建模

熵编码作为无损压缩的核心环节，目标是以最短平均码长表示信源符号。霍夫曼编码与算术编码是常用方法。深度学习压缩模型通常结合概率建模与熵编码，对潜在表示进行高效编码，进一步降低比特率。🔄

在熵建模方面，研究持续深入。高级自回归熵模型如高斯混合模型（GMM）[8]和高斯-拉普拉斯-Logistic混合模型（GLLMM）[11]，能够更精确地估计潜在变量的概率分布，但其顺序特性导致计算量大，难以充分利用GPU并行加速。为提升效率，学者们提出了可并行化的熵模型（如棋盘上下文模型[15]）和通道自回归熵模型（ChARM[33]），在压缩性能与解码速度之间取得平衡。🔄

## 2.4 架构创新与效率优化

近年来，架构创新成为提升LIC性能的重要驱动力。残差网络[8,11]、可逆神经网络[43]、倍频程卷积模块[7]等结构被引入以增强模型表达能力。与此同时，Transformer模块的集成[35,47,29]显著提升了全局建模能力，尽管压缩效果优异，但对计算资源的需求也随之增加，尤其在大尺寸输入下，复杂度呈二次增长，对GPU内存提出更高要求。🔄

🔄此外，注意力机制的不断演化也推动了架构创新。部分研究将注意力模块与其他结构结合，进一步提升了模型对图像细节的捕捉能力。例如，窗口化注意力机制能够在保证计算效率的同时，增强模型对局部区域的关注，从而优化速率失真表现。

## 2.5 知识蒸馏（Knowledge Distillation）

知识蒸馏作为模型压缩的重要手段，通过让小型学生模型学习大型教师模型的知识，实现模型轻量化的同时保持性能。蒸馏内容可涵盖输出分布、中间特征、注意力图等。在深度图像压缩领域，知识蒸馏有助于提升轻量模型的压缩效率与重建质量，便于在资源受限设备上部署。🔄

🔄知识蒸馏最初由Hinton等人提出[20]，其核心思想是训练轻量级学生网络以模拟大型教师模型的输出。该方法已广泛应用于计算机视觉任务[6,45,14,27,46]，如对象检测领域的FGD方法[44]和图像分类中的多种蒸馏技术[40,19]，均取得了显著成果。

🔄然而，在LIC领域，知识蒸馏的探索相对有限。部分工作[18]尝试结合GAN进行蒸馏，关注低比特率下的视觉表现，但未充分利用超网络和中间特征。Fu等人[12]提出的蒸馏方法同时考虑了潜在表示的输出与概率分布，有效降低了解码器复杂度，但未利用中间特征知识，且教师与学生网络共用编码器，导致优化过程复杂，解码复杂性降低问题未得到根本解决。

---
*注：本章内容为初步梳理，后续将根据研究进展和文献阅读进一步完善。*